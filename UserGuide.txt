Tips for Using the OverApprox Python Repo

This repo is organized to allow the user to train neural networks on the inverted pendulum environment,
simulate the learned policies, contactenate the control network with an overapproximated dynamics network,
and test with Marabou.

In this project we are interested in training FFNNs, CNNs, and RNNs. 
To train a FFNN, use the file train_pendulum_agent.py
To train a CNN, use the file train_pendulum_from_images.py
I don't have a good way to train RNNs yet.

Lasagne is installed this way: pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip
NNet install: https://github.com/sisl/NNet/tree/654ca0b2393f3057c06e53a16f2aab6ccf707b48

This training will output a ".pkl" file among others.
The file good_policy_to_nnv.py takes this file and outputs a ".nnv" file from which layers, weights, and biases are logged.
The trained controller can be simulated on the environment using the file sim_varying.py
Simulation parameters can be tweaked in the gym environment my_pendulum.py (my_pendulum_images.py for convolutional nets)
The starting seed can be made constant to always have the simulation start from the same spot.

To concatenate the learned controller with overapproximated dynamics, use the file run_overapprox_piecewise.py
To view the resulting graph in tensorboard, navigate to the OverApprox folder, then run $tensorboard --logdir="./tensorboard_logs" --port 6006
I haven't been able to concatenate a CNN (or RNN for that matter) with the dynamics network yet.

The file usingmaraboupy.py is a script that runs marabou. 
It loads the concatenated network and constructs input output bounds and then passes things to marabou.
Before using, need to build maraboupy. See the Marabou github for this.

